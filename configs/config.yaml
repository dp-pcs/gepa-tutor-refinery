# Experiment configuration
experiment_name: truthfulqa_official
seed: 42

dataset:
  # one of: synthetic | race | arc_easy | arc_challenge
  name: truthfulqa_official
  subset: "middle"   # only for race: middle|high
  n_train: 30
  n_dev: 10
  n_test: 10

model:
  # provider: mock | openai | anthropic
  # Set to "openai" or "anthropic" to use real LLMs
  # Make sure to set OPENAI_API_KEY or ANTHROPIC_API_KEY environment variables
  provider: "openai"  # Using OpenAI with API key from .env
  
  # OpenAI models: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
  # Anthropic models: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
  model_id: "gpt-3.5-turbo"  # Using 3.5-turbo for more realistic challenge
  
  temperature: 0.2
  max_output_tokens: 256
  request_timeout: 60

evaluation:
  strategy: "baseline"   # overridden by CLI --mode
  self_refine_steps: 1
  # metrics we log automatically: accuracy, tokens_out, latency_sec

gepa:
  num_reflection_examples: 20      # how many failed examples to include
  num_edits: 3                     # how many edits to propose per round
  max_rounds: 2
  pareto_metric_x: "avg_tokens_out"
  pareto_metric_y: "accuracy"

logging:
  runs_dir: "runs"

# Environment Variables Required:
# For OpenAI: export OPENAI_API_KEY="your-api-key-here"
# For Anthropic: export ANTHROPIC_API_KEY="your-api-key-here"
# 
# To switch providers, change the "provider" field above and ensure the corresponding
# API key is set in your environment.
